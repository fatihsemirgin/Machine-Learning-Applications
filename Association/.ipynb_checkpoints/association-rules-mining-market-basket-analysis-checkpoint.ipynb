{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "450fd31f-55ff-435d-b420-8a8cd33ea7c9",
    "_uuid": "4c788bfac2c397099c4eaa99656a7a83036b3d45"
   },
   "source": [
    "# Association Rules Mining Using Python Generators to Handle Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a86ef7fc-2d2a-4ba8-b4c7-f7c9782e084c",
    "_uuid": "520e62e6bec439a67d5790c48960363d69d74b99"
   },
   "source": [
    "### Motivation\n",
    "I was looking to run association analysis in Python using the apriori algorithm to derive rules of the form {A} -> {B}.  However, I quickly discovered that it's not part of the standard Python machine learning libraries.  Although there are some implementations that exist, I could not find one capable of handling large datasets.  \"Large\" in my case was an orders dataset with 32 million records, containing 3.2 million unique orders and about 50K unique items (file size just over 1 GB).  So, I decided to write my own implementation, leveraging the apriori algorithm to generate simple {A} -> {B} association rules. Since I only care about understanding relationships between any given pair of items, using apriori to get to item sets of size 2 is sufficient.  I went through various iterations, splitting the data into multiple subsets just so I could get functions like crosstab and combinations to run on my machine with 8 GB of memory.  :)  But even with this approach, I could only process about 1800 items before my kernel would crash...  And that's when I learned about the wonderful world of Python generators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cf99a98-f1f2-4666-a39e-aa923c3a0c07",
    "_uuid": "aa1d4de4449ca66d8d315d8df6d20fc12c4f82e7"
   },
   "source": [
    "### Python Generators\n",
    "\n",
    "In a nutshell, a generator is a special type of function that returns an iterable sequence of items.  However, unlike regular functions which return all the values at once (eg: returning all the elements of a list), a generator <i>yields</i> one value at a time.  To get the next value in the set, we must ask for it - either by explicitly calling the generator's built-in \"next\" method, or implicitly via a for loop.  This is a great property of generators because it means that we don't have to store all of the values in memory at once.  We can load and process one value at a time, discard when finished and move on to process the next value.  This feature makes generators perfect for creating item pairs and counting their frequency of co-occurence.  Here's a concrete example of what we're trying to accomplish:  \n",
    "\n",
    "1. Get all possible item pairs for a given order \n",
    "       eg:  order 1:  apple, egg, milk   -->  item pairs: {apple, egg}, {apple, milk}, {egg, milk}\n",
    "            order 2:  egg, milk          -->  item pairs: {egg, milk}\n",
    "            \n",
    "2. Count the number of times each item pair appears\n",
    "       eg: {apple, egg}: 1\n",
    "           {apple, milk}: 1\n",
    "           {egg, milk}: 2\n",
    "\n",
    "Here's the generator that implements the above tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "cac0e787-9263-4d31-b070-eb1742928782",
    "_uuid": "a16756678652a9f0108e88c238bd23ca6658e0ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('apple', 'egg'): 1, ('apple', 'milk'): 1, ('egg', 'milk'): 2})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "\n",
    "# Sample data\n",
    "orders = np.array([[1,'apple'], [1,'egg'], [1,'milk'], [2,'egg'], [2,'milk']], dtype=object)\n",
    "\n",
    "# Generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    \n",
    "    # For each order, generate a list of items in that order\n",
    "    for order_id, order_object in groupby(orders, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]      \n",
    "    \n",
    "        # For each item list, generate item pairs, one at a time\n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair                                      \n",
    "\n",
    "\n",
    "# Counter iterates through the item pairs returned by our generator and keeps a tally of their occurrence\n",
    "Counter(get_item_pairs(orders))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73cd2676-7749-47c3-b9c9-0da9c93c0790",
    "_uuid": "f9359858a4df545cb88f989cce45092a3a89df2b"
   },
   "source": [
    "<i>get_item_pairs()</i> generates a list of items for each order and produces item pairs for that order, one pair at a time.  The first item pair is passed to Counter which keeps track of the number of times an item pair occurs.  The next item pair is taken, and again, passed to Counter.  This process continues until there are no more item pairs left.  With this approach, we end up not using much memory as item pairs are discarded after the count is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e7552ed1-fcfe-422b-a76c-f1bf91a2e064",
    "_uuid": "2bfe59ecf1564e278d73bee93e222372f0c01dd3"
   },
   "source": [
    "### Apriori Algorithm \n",
    "Apriori is an algorithm used to identify frequent item sets (in our case, item pairs).  It does so using a \"bottom up\" approach, first identifying individual items that satisfy a minimum occurence threshold. It then extends the item set, adding one item at a time and checking if the resulting item set still satisfies the specified threshold.  The algorithm stops when there are no more items to add that meet the minimum occurrence requirement.  Here's an example of apriori in action, assuming a minimum occurence threshold of 3:\n",
    "\n",
    "\n",
    "    order 1: apple, egg, milk  \n",
    "    order 2: carrot, milk  \n",
    "    order 3: apple, egg, carrot\n",
    "    order 4: apple, egg\n",
    "    order 5: apple, carrot\n",
    "\n",
    "    \n",
    "    Iteration 1:  Count the number of times each item occurs   \n",
    "    item set      occurrence count    \n",
    "    {apple}              4   \n",
    "    {egg}                3   \n",
    "    {milk}               2   \n",
    "    {carrot}             2   \n",
    "\n",
    "    {milk} and {carrot} are eliminated because they do not meet the minimum occurrence threshold.\n",
    "\n",
    "\n",
    "    Iteration 2: Build item sets of size 2 using the remaining items from Iteration 1 \n",
    "                 (ie: apple, egg)  \n",
    "    item set           occurence count  \n",
    "    {apple, egg}             3  \n",
    "\n",
    "    Only {apple, egg} remains and the algorithm stops since there are no more items to add.\n",
    "   \n",
    "   \n",
    "If we had more orders and items, we can continue to iterate, building item sets consisting of more than 2 elements.  For the problem we are trying to solve (ie: finding relationships between pairs of items), it suffices to implement apriori to get to item sets of size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9814b186-7467-4fbe-b8e8-34adf970cfdc",
    "_uuid": "516cfe4664132e59fbbcd1d4742643ab58f21b29"
   },
   "source": [
    "### Association Rules Mining\n",
    "Once the item sets have been generated using apriori, we can start mining association rules.  Given that we are only looking at item sets of size 2, the association rules we will generate will be of the form {A} -> {B}.  One common application of these rules is in the domain of recommender systems, where customers who purchased item A are recommended item B.\n",
    "\n",
    "Here are 3 key metrics to consider when evaluating association rules:\n",
    "\n",
    "1. <b>support</b>  \n",
    "    This is the percentage of orders that contains the item set. In the example above, there are 5 orders in total \n",
    "    and {apple,egg} occurs in 3 of them, so: \n",
    "       \n",
    "                    support{apple,egg} = 3/5 or 60%\n",
    "        \n",
    "    The minimum support threshold required by apriori can be set based on knowledge of your domain.  In this \n",
    "    grocery dataset for example, since there could be thousands of distinct items and an order can contain \n",
    "    only a small fraction of these items, setting the support threshold to 0.01% may be reasonable.<br><br><br>\n",
    "    \n",
    "2. <b>confidence</b>  \n",
    "    Given two items, A and B, confidence measures the percentage of times that item B is purchased, given that \n",
    "    item A was purchased. This is expressed as:\n",
    "       \n",
    "                    confidence{A->B} = support{A,B} / support{A}   \n",
    "                    \n",
    "    Confidence values range from 0 to 1, where 0 indicates that B is never purchased when A is purchased, and 1 \n",
    "    indicates that B is always purchased whenever A is purchased.  Note that the confidence measure is directional.     This means that we can also compute the percentage of times that item A is purchased, given that item B was \n",
    "    purchased:\n",
    "       \n",
    "                    confidence{B->A} = support{A,B} / support{B}    \n",
    "                    \n",
    "    In our example, the percentage of times that egg is purchased, given that apple was purchased is:  \n",
    "       \n",
    "                    confidence{apple->egg} = support{apple,egg} / support{apple}\n",
    "                                           = (3/5) / (4/5)\n",
    "                                           = 0.75 or 75%\n",
    "\n",
    "    A confidence value of 0.75 implies that out of all orders that contain apple, 75% of them also contain egg.  Now, \n",
    "    we look at the confidence measure in the opposite direction (ie: egg->apple): \n",
    "       \n",
    "                    confidence{egg->apple} = support{apple,egg} / support{egg}\n",
    "                                           = (3/5) / (3/5)\n",
    "                                           = 1 or 100%  \n",
    "                                           \n",
    "    Here we see that all of the orders that contain egg also contain apple.  But, does this mean that there is a \n",
    "    relationship between these two items, or are they occurring together in the same orders simply by chance?  To \n",
    "    answer this question, we look at another measure which takes into account the popularity of <i>both</i> items.<br><br><br>  \n",
    "    \n",
    "3. <b>lift</b>  \n",
    "    Given two items, A and B, lift indicates whether there is a relationship between A and B, or whether the two items \n",
    "    are occuring together in the same orders simply by chance (ie: at random).  Unlike the confidence metric whose \n",
    "    value may vary depending on direction (eg: confidence{A->B} may be different from confidence{B->A}), \n",
    "    lift has no direction. This means that the lift{A,B} is always equal to the lift{B,A}: \n",
    "       \n",
    "                    lift{A,B} = lift{B,A} = support{A,B} / (support{A} * support{B})   \n",
    "    \n",
    "    In our example, we compute lift as follows:\n",
    "    \n",
    "         lift{apple,egg} = lift{egg,apple} = support{apple,egg} / (support{apple} * support{egg})\n",
    "                         = (3/5) / (4/5 * 3/5) \n",
    "                         = 1.25    \n",
    "               \n",
    "    One way to understand lift is to think of the denominator as the likelihood that A and B will appear in the same \n",
    "    order if there was <i>no</i> relationship between them. In the example above, if apple occurred in 80% of the\n",
    "    orders and egg occurred in 60% of the orders, then if there was no relationship between them, we would \n",
    "    <i>expect</i> both of them to show up together in the same order 48% of the time (ie: 80% * 60%).  The numerator, \n",
    "    on the other hand, represents how often apple and egg <i>actually</i> appear together in the same order.  In \n",
    "    this example, that is 60% of the time.  Taking the numerator and dividing it by the denominator, we get to how \n",
    "    many more times apple and egg actually appear in the same order, compared to if there was no relationship between     them (ie: that they are occurring together simply at random).  \n",
    "    \n",
    "    In summary, lift can take on the following values:\n",
    "    \n",
    "        * lift = 1 implies no relationship between A and B. \n",
    "          (ie: A and B occur together only by chance)\n",
    "      \n",
    "        * lift > 1 implies that there is a positive relationship between A and B.\n",
    "          (ie:  A and B occur together more often than random)\n",
    "    \n",
    "        * lift < 1 implies that there is a negative relationship between A and B.\n",
    "          (ie:  A and B occur together less often than random)\n",
    "        \n",
    "    In our example, apple and egg occur together 1.25 times <i>more</i> than random, so we conclude that there exists \n",
    "    a positive relationship between them.\n",
    "   \n",
    "Armed with knowledge of apriori and association rules mining, let's dive into the data and code to see what relationships we unravel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "719e883c-03f3-4fb1-90e3-fd99855ff022",
    "_uuid": "338d54d8ba644a93dd8d5076d5edf3fa24121006"
   },
   "source": [
    "### Input Dataset\n",
    "Instacart, an online grocer, has graciously made some of their datasets accessible to the public.  The order and product datasets that we will be using can be downloaded from the link below, along with the data dictionary:\n",
    "\n",
    "“The Instacart Online Grocery Shopping Dataset 2017”, Accessed from https://www.instacart.com/datasets/grocery-shopping-2017 on September 1, 2017.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6187d6e8-3a64-4802-b5a5-c8dcf828a0b3",
    "_uuid": "0e487b731f49e05e7f81490e36894ade9fdd3f50"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "ebb38b54-3250-4636-a53e-6c422f4456bf",
    "_uuid": "009e16e2fcdf0810f77627d1c6439ac5bcc17377"
   },
   "outputs": [],
   "source": [
    "# Function that returns the size of an object in MB\n",
    "def size(obj):\n",
    "    return \"{0:.2f} MB\".format(sys.getsizeof(obj) / (1000 * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ddefe399-1b17-415b-bad2-b4dfcc9c734f",
    "_uuid": "d92f5eb045da7e74b902fda4ba430b247100bd6f"
   },
   "source": [
    "### Part 1:  Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "611c8c1a-69df-4e82-8e7f-509ed8687af3",
    "_uuid": "98ca92d66cc1aa6d545d9e414971c718b0801e70"
   },
   "source": [
    "#### A. Load order  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "17379a10-59fc-40ad-8dbb-13c27889094e",
    "_uuid": "7f3836b73419e5f8957f9d42500c40ee8f457f0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders -- dimensions: (32434489, 4);   size: 1037.90 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>33120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28985</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9327</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>45918</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>30035</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  add_to_cart_order  reordered\n",
       "0         2       33120                  1          1\n",
       "1         2       28985                  2          1\n",
       "2         2        9327                  3          0\n",
       "3         2       45918                  4          1\n",
       "4         2       30035                  5          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orders = pd.read_csv('order_products__prior.csv')\n",
    "print('orders -- dimensions: {0};   size: {1}'.format(orders.shape, size(orders)))\n",
    "display(orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92ac9b55-e7b7-4cfe-8d70-4ab601226a88",
    "_uuid": "94fdf6593d6d766c862f9d03e1172949ef9c6e8b"
   },
   "source": [
    "#### B. Convert order data into format expected by the association rules function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "e9736090-913e-4963-b381-7b24c6a70d00",
    "_uuid": "072c8a04a5f9488573f5082c01df7e809868dd15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id\n",
       "2    33120\n",
       "2    28985\n",
       "2     9327\n",
       "2    45918\n",
       "2    30035\n",
       "2    17794\n",
       "2    40141\n",
       "2     1819\n",
       "2    43668\n",
       "3    33754\n",
       "Name: item_id, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert from DataFrame to a Series, with order_id as index and item_id as value\n",
    "orders = orders.set_index('order_id')['product_id'].rename('item_id')\n",
    "display(orders.head(10))\n",
    "type(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3f20905-3b6a-4ff5-bdc2-ae8b92c069e5",
    "_uuid": "5cd89dbd29af1eb5d5d3cca36eb8e11b26431164"
   },
   "source": [
    "#### C. Display summary statistics for order data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "37d2f26c-6398-4544-8ac1-0afa21d066a3",
    "_uuid": "0950199479ff1e56263f66e4b171ff45379e03b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions: (32434489,);   size: 518.95 MB;   unique_orders: 3214874;   unique_items: 49677\n"
     ]
    }
   ],
   "source": [
    "print('dimensions: {0};   size: {1};   unique_orders: {2};   unique_items: {3}'\n",
    "      .format(orders.shape, size(orders), len(orders.index.unique()), len(orders.value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "afa98af1-9a11-43ce-8e12-014c6e724075",
    "_uuid": "697a072213997ed4e7c8affbfc0b1ea736113aac"
   },
   "source": [
    "### Part 2: Association Rules Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7efcec16-16a8-407f-bd9a-6d53c491c01a",
    "_uuid": "1f5345c3bff3b50a35fec45ffa81838e24d134d3"
   },
   "source": [
    "#### A. Helper functions to the main association rules function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "95729dab-8f56-4ec4-8d12-15ed38eff18d",
    "_uuid": "55ba9d736129af7561f275d3dc29d8a3b2dfd30e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Returns frequency counts for items and item pairs\n",
    "def freq(iterable):\n",
    "    if type(iterable) == pd.core.series.Series:\n",
    "        return iterable.value_counts().rename(\"freq\")\n",
    "    else: \n",
    "        return pd.Series(Counter(iterable)).rename(\"freq\")\n",
    "\n",
    "    \n",
    "# Returns number of unique orders\n",
    "def order_count(order_item):\n",
    "    return len(set(order_item.index))\n",
    "\n",
    "\n",
    "# Returns generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    order_item = order_item.reset_index().as_matrix()\n",
    "    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "              \n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "            \n",
    "\n",
    "# Returns frequency and support associated with item\n",
    "def merge_item_stats(item_pairs, item_stats):\n",
    "    return (item_pairs\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n",
    "\n",
    "\n",
    "# Returns name associated with item\n",
    "def merge_item_name(rules, item_name):\n",
    "    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n",
    "               'confidenceAtoB','confidenceBtoA','lift']\n",
    "    rules = (rules\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n",
    "    return rules[columns]\n",
    "\n",
    "\n",
    "\n",
    "def association_rules(order_item, min_support):\n",
    "\n",
    "    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Calculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Filter from order_item items below min support \n",
    "    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n",
    "    order_item             = order_item[order_item.isin(qualifying_items)]\n",
    "\n",
    "    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Filter from order_item orders with less than 2 items\n",
    "    order_size             = freq(order_item.index)\n",
    "    qualifying_orders      = order_size[order_size >= 2].index\n",
    "    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n",
    "\n",
    "    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Recalculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Get item pairs generator\n",
    "    item_pair_gen          = get_item_pairs(order_item)\n",
    "\n",
    "\n",
    "    # Calculate item pair frequency and support\n",
    "    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n",
    "    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) * 100\n",
    "\n",
    "    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Filter from item_pairs those below min support\n",
    "    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n",
    "\n",
    "    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Create table of association rules and compute relevant metrics\n",
    "    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    item_pairs = merge_item_stats(item_pairs, item_stats)\n",
    "    \n",
    "    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n",
    "    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n",
    "    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n",
    "    \n",
    "    \n",
    "    # Return association rules sorted by lift in descending order\n",
    "    return item_pairs.sort_values('lift', ascending=False)\n",
    "\n",
    "\n",
    "rules = association_rules(orders, 0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17b82fe5-464d-422b-8b4e-550551b54e58",
    "_uuid": "fadb8d35353ee366b37d22f051f6c5d670704b96"
   },
   "source": [
    "#### B. Association rules function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e94b8466-b7db-4feb-b27a-b65e37626bac",
    "_uuid": "ebde898bf77e55b5ef8643d89ce961c1aa8ec08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting order_item:               32434489\n",
      "Items with support >= 0.01:           10906\n",
      "Remaining order_item:              29843570\n",
      "Remaining orders with 2+ items:     3013325\n",
      "Remaining order_item:              29662716\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\magics\\execution.py\", line 1325, in time\n",
      "    exec(code, glob, local_ns)\n",
      "  File \"<timed exec>\", line 63, in <module>\n",
      "  File \"<timed exec>\", line 38, in association_rules\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_29944\\985104517.py\", line 6, in freq\n",
      "    return pd.Series(Counter(iterable)).rename(\"freq\")\n",
      "  File \"c:\\python38\\lib\\collections\\__init__.py\", line 552, in __init__\n",
      "    self.update(iterable, **kwds)\n",
      "  File \"c:\\python38\\lib\\collections\\__init__.py\", line 637, in update\n",
      "    _count_elements(self, iterable)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_29944\\985104517.py\", line 16, in get_item_pairs\n",
      "    order_item = order_item.reset_index().as_matrix()\n",
      "  File \"c:\\python38\\lib\\site-packages\\pandas\\core\\generic.py\", line 5989, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'as_matrix'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1122, in get_records\n",
      "    FrameInfo(\n",
      "  File \"c:\\python38\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 766, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "  File \"c:\\python38\\lib\\inspect.py\", line 967, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"c:\\python38\\lib\\inspect.py\", line 798, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e608b37c-1872-4632-b0ec-307b3ff19a0c",
    "_uuid": "9cfdef69fb7abc9927cbab401b1a6cab14b494f9"
   },
   "source": [
    "### Part 3:  Association Rules Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09be74c2-1a24-48ff-bbcf-ca701b0abe4e",
    "_uuid": "bc848efd05034abd042467e15388d8a179e6b8a8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "af1e9b6b-6191-43aa-a0b8-40a3abf49073",
    "_uuid": "b94f6796164b5915d923b71117e9322e7a4afa41"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m item_name   \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m item_name   \u001b[38;5;241m=\u001b[39m item_name\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_name\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 4\u001b[0m rules_final \u001b[38;5;241m=\u001b[39m merge_item_name(\u001b[43mrules\u001b[49m, item_name)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlift\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m display(rules_final)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rules' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace item ID with item name and display association rules\n",
    "item_name   = pd.read_csv('products.csv')\n",
    "item_name   = item_name.rename(columns={'product_id':'item_id', 'product_name':'item_name'})\n",
    "rules_final = merge_item_name(rules, item_name).sort_values('lift', ascending=False)\n",
    "display(rules_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cae6d63e-8a21-42cb-8f66-18c4fed5bcf4",
    "_uuid": "4a4692d044fb1977315f38165d47955a21cd054f"
   },
   "source": [
    "### Part 4:  Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "344d7ef3-953c-48f3-8c64-8f2cb5413341",
    "_uuid": "1fb820fbf9612419e59483bab6d37fb0bccd21f4"
   },
   "source": [
    "From the output above, we see that the top associations are not surprising, with one flavor of an item being purchased with another flavor from the same item family (eg: Strawberry Chia Cottage Cheese with Blueberry Acai Cottage Cheese, Chicken Cat Food with Turkey Cat Food, etc).  As mentioned, one common application of association rules mining is in the domain of recommender systems.  Once item pairs have been identified as having positive relationship, recommendations can be made to customers in order to increase sales.  And hopefully, along the way, also introduce customers to items they never would have tried before or even imagined existed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
